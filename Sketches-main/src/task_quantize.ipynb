{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final project - 5/3/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import Quantizer\n",
    "import quantizationItamar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1:\n",
    "Helper class to single conv layer\n",
    "note: we add bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedConvLayer(nn.Module):\n",
    "    def __init__(self, quantized_weights, original_shape, bias=False):\n",
    "        super().__init__()\n",
    "        self.weights = torch.from_numpy(quantized_weights).float().view(original_shape)\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.ones(original_shape[0]))  # אתחול הביאס ל-1\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.conv2d(x, self.weights, bias=self.bias, stride=1, padding=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2:\n",
    "Single Layer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# יצירת מודל עם שכבה אחת\n",
    "class SingleLayerModel(nn.Module):\n",
    "    def __init__(self, quantized_weights, original_shape, bias=True):\n",
    "        # Initialize the object with quantized weights and other parameters\n",
    "        self.quantized_weights = quantized_weights  # Store quantized_weights in the instance\n",
    "        self.original_shape = original_shape\n",
    "        self.bias = bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Task 3:\n",
    "Randome vector- quantized and dequantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average relative error: 0.04171245743774483\n",
      "Maximum relative error: 1.0\n"
     ]
    }
   ],
   "source": [
    "# יצירת וקטור לדוגמה לבדיקת הכימות\n",
    "vec = np.random.randn(100).astype(np.float32)  # משקלים אקראיים בגודל 3x3x3, 64 פילטרים\n",
    "\n",
    "original_shape = (10,10)\n",
    "\n",
    "# כימות הווקטור\n",
    "grid = quantizationItamar.generate_grid(8, False)\n",
    "quantized_weights, scale, z = Quantizer.quantize(vec=vec, grid=grid)\n",
    "\n",
    "# השוואת וקטורים לפני ואחרי הכימות\n",
    "dequantized_weights = Quantizer.dequantize(quantized_weights, scale, z)\n",
    "\n",
    "vector_difference = np.abs(vec - dequantized_weights.flatten()).mean()\n",
    "\n",
    "# חישוב השגיאה היחסית והמקסימלית\n",
    "relative_errors = np.abs((vec - dequantized_weights.flatten()) / vec)\n",
    "relative_errors = np.where(np.isfinite(relative_errors), relative_errors, 0)  # טיפול במקרים של חלוקה באפס\n",
    "\n",
    "average_relative_error = np.mean(relative_errors)\n",
    "max_relative_error = np.max(relative_errors)\n",
    "\n",
    "print(\"Average relative error:\", average_relative_error)\n",
    "print(\"Maximum relative error:\", max_relative_error)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the results into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectors saved to vectors_output.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"vectors_output.txt\", \"w\") as f:\n",
    "    f.write(\"Original vector:\\n\")\n",
    "    np.savetxt(f, vec.reshape(-100, 100), fmt=\"%.6f\")\n",
    "    f.write(\"\\nDequantized vector:\\n\")\n",
    "    np.savetxt(f, dequantized_weights.reshape(-100, 100), fmt=\"%.6f\")\n",
    "    f.write(\"\\nvector_difference:\\n\")\n",
    "    np.savetxt(f, vector_difference.reshape(-1, 1), fmt=\"%.6f\")\n",
    "print(\"Vectors saved to vectors_output.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output from file:\n",
    "\n",
    "Original vector:\n",
    "-0.111487 0.850883 -1.562684 -0.561151 0.165896 -0.616522 -0.294702 -0.178508 -0.119455 0.856403 -0.005942 -0.944988 -0.185713 -1.316645 0.863703 0.146863 2.823082 0.450654 -1.708542 -0.476596 1.867064 0.389970 -1.967263 -0.890203 0.928423 0.202063 0.858497 -0.375497 0.413610 -0.777870 -0.032337 1.293657 0.966133 -0.319092 -1.100120 0.950536 -0.845570 1.726389 1.885617 1.256837 1.217571 -0.203107 1.917088 0.126608 -0.931344 -1.840507 -1.579605 -0.550749 -0.427611 -0.631418 2.302883 1.569178 0.134905 -0.141346 -1.992737 0.787142 0.997367 1.543908 1.267690 -0.805269 0.919002 0.291717 0.560167 -0.341881 -1.454660 -1.803350 1.644349 1.351046 -0.138112 -1.322578 0.343876 -0.676363 -0.518376 0.261224 1.250156 1.880378 -0.944845 -0.257316 -0.693713 0.533303 0.203999 -0.596182 0.611321 -0.632613 0.697375 -0.829484 0.523490 0.563056 -0.220699 -0.058949 0.108626 0.239777 -0.356313 0.832974 0.551994 0.847768 -2.287619 -0.387235 0.229343 1.890911\n",
    "\n",
    "Dequantized vector:\n",
    "-0.100210 0.841762 -1.543231 -0.541133 0.160336 -0.601259 -0.280587 -0.160336 -0.100210 0.841762 0.000000 -0.941972 -0.180378 -1.302728 0.861804 0.140294 2.805875 0.440923 -1.703567 -0.460965 1.863903 0.380797 -1.964112 -0.881846 0.921930 0.200420 0.841762 -0.360755 0.400839 -0.761595 -0.020042 1.282686 0.962014 -0.300629 -1.082266 0.941972 -0.841762 1.723609 1.883945 1.242602 1.202518 -0.200420 1.903987 0.120252 -0.921930 -1.823819 -1.563273 -0.541133 -0.420881 -0.621301 2.284784 1.563273 0.120252 -0.140294 -1.984154 0.781637 0.982056 1.543231 1.262644 -0.801679 0.901888 0.280587 0.541133 -0.340713 -1.443021 -1.783735 1.643441 1.342812 -0.120252 -1.302728 0.340713 -0.661385 -0.501049 0.260546 1.242602 1.863903 -0.941972 -0.240504 -0.681427 0.521091 0.200420 -0.581217 0.601259 -0.621301 0.681427 -0.821720 0.521091 0.561175 -0.220462 -0.040084 0.100210 0.220462 -0.340713 0.821720 0.541133 0.841762 -2.284784 -0.380797 0.220462 1.883945\n",
    "\n",
    "vector_difference:\n",
    "0.010174\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4:\n",
    "One layer network and quntize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector difference (mean absolute error): 0.009622683358725714\n",
      "Average relative error: 0.04171245743774483\n",
      "Maximum relative error: 1.0\n"
     ]
    }
   ],
   "source": [
    "# do quantized\n",
    "quantized_weights, scale, z = Quantizer.quantize(vec=vec, grid=grid)\n",
    "\n",
    "# Initialize model with quantized weights\n",
    "model = SingleLayerModel(quantized_weights, original_shape, bias=True)\n",
    "\n",
    "# Dequantizing weights\n",
    "dequantized_weights = Quantizer.dequantize(model.quantized_weights , scale, z)\n",
    "\n",
    "# Calculate vector difference mean\n",
    "vector_difference = np.abs(vec - dequantized_weights.flatten()).mean()\n",
    "\n",
    "# Calculate relative \n",
    "errors = np.abs((vec - dequantized_weights.flatten()) / vec)\n",
    "\n",
    "# Handling NaNs or infinities in relative_errors (for division by zero)\n",
    "relative_errors = np.where(np.isfinite(relative_errors), relative_errors, 0)\n",
    "\n",
    "# Compute average and maximum errors\n",
    "average_error = np.mean(relative_errors)\n",
    "max_error = np.max(relative_errors)\n",
    "\n",
    "# Print the results\n",
    "print(\"Vector difference (mean absolute error):\", vector_difference)\n",
    "print(\"Average relative error:\", average_error)\n",
    "print(\"Maximum relative error:\", max_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: genrate vectors with diffrente range etc \n",
    "### Examine the results of each type of vector\n",
    "function 1: create_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors():\n",
    "    # Randome vector\n",
    "    vec_random = np.random.randn(100).astype(np.float32)\n",
    "    \n",
    "    # pos velue\n",
    "    vec_positive = np.abs(np.random.randn(100).astype(np.float32))\n",
    "    \n",
    "    #vector with valus in range [-10,10]\n",
    "    vec_range = np.random.uniform(-10, 10, 100).astype(np.float32)\n",
    "    \n",
    "    # וקטור עם ערכים מאוד קטנים (כדי לבדוק את דיוק הקוונטיזציה)\n",
    "    vec_small = np.random.randn(100).astype(np.float32) * 1e-6\n",
    "    \n",
    "    return vec_random, vec_positive, vec_range, vec_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file to write the results to\n",
    "with open('quantization_results.txt', 'w') as file:\n",
    "    \n",
    "    # For each vector, perform quantization, dequantization and evaluation\n",
    "\n",
    "    for vec in vectors:\n",
    "        # Write the vector type to the file\n",
    "        file.write(f\"Processing vector with values:\\n{vec}...\\n\")  # Print part of the vector\n",
    "        \n",
    "        # Generate the grid for quantization\n",
    "        grid = quantizationItamar.generate_grid(8, True)\n",
    "\n",
    "        # Perform quantization\n",
    "        quantized_weights, scale, z = Quantizer.quantize(vec=vec, grid=grid)\n",
    "\n",
    "        # Create model with quantized weights\n",
    "        model = SingleLayerModel(quantized_weights, original_shape=(10, 10), bias=True)\n",
    "\n",
    "        # Perform dequantization\n",
    "        dequantized_weights = Quantizer.dequantize(model.quantized_weights, scale, z)\n",
    "\n",
    "        # Write results to the file\n",
    "        file.write(f\"dequantized_weights: {dequantized_weights}\\n\")\n",
    "\n",
    "        # Write intermediate results to the file (vector difference, etc.)\n",
    "        vector_difference = np.abs(vec - dequantized_weights.flatten()).mean()\n",
    "\n",
    "        # Calculate relative errors\n",
    "        relative_errors = np.abs((vec - dequantized_weights.flatten()) / vec)\n",
    "        relative_errors = np.where(np.isfinite(relative_errors), relative_errors, 0)  # Handle division by zero\n",
    "\n",
    "        # Calculate average and maximum relative errors\n",
    "        average_relative_error = np.mean(relative_errors)\n",
    "        max_relative_error = np.max(relative_errors)\n",
    "\n",
    "        # Write results to the file\n",
    "        file.write(f\"Vector difference (mean absolute error): {vector_difference}\\n\")\n",
    "        file.write(f\"Average relative error: {average_relative_error}\\n\")\n",
    "        file.write(f\"Maximum relative error: {max_relative_error}\\n\\n\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 1:\n",
    "###  For a Randome vector:\n",
    "Processing vector with values:\n",
    "[ 1.845 -0.024  0.657 -1.122  0.105 -0.563  0.009 -1.249  1.207 -0.408\n",
    "  1.842  1.724  1.019  0.537 -1.06  -0.054 -1.125 -0.246 -1.27   0.953\n",
    " -0.697 -0.809 -0.855 -1.088 -1.315 -0.516  0.362 -0.311 -1.889  1.169\n",
    " -0.067 -0.749  1.73   0.041 -0.83   0.223  1.78   0.438  0.686 -0.163\n",
    " -0.727 -1.002  0.992  2.056 -1.237  1.357  1.3   -0.996  0.375 -0.469\n",
    "  0.912 -0.283  0.868  1.727  0.022 -0.386  1.109  0.573  0.217 -0.621\n",
    "  0.828 -2.017  0.235  1.48  -1.314 -0.152  0.035  0.403  1.     0.301\n",
    " -0.225  1.083 -0.268  0.379 -0.723  0.899  2.327  0.471 -0.244  2.279\n",
    " -0.26  -0.105 -2.934  1.683 -1.04   1.013  1.58  -1.523  0.919 -1.786\n",
    "  0.55   1.073  0.158 -1.12  -0.606 -0.656 -0.51  -1.438 -0.295  1.27 ]\n",
    "\n",
    "\n",
    "dequantized_weights: [ 1.836 -0.021  0.64  -1.114  0.103 -0.557  0.    -1.238  1.197 -0.392\n",
    "  1.836  1.713  1.011  0.536 -1.052 -0.041 -1.114 -0.227 -1.259  0.949\n",
    " -0.681 -0.805 -0.846 -1.073 -1.3   -0.516  0.351 -0.31  -1.878  1.156\n",
    " -0.062 -0.743  1.713  0.021 -0.825  0.206  1.775  0.433  0.681 -0.144\n",
    " -0.722 -0.99   0.99   2.043 -1.217  1.341  1.3   -0.99   0.371 -0.454\n",
    "  0.908 -0.268  0.867  1.713  0.021 -0.371  1.094  0.557  0.206 -0.619\n",
    "  0.825 -2.002  0.227  1.465 -1.3   -0.144  0.021  0.392  0.99   0.289\n",
    " -0.206  1.073 -0.248  0.371 -0.722  0.887  2.311  0.454 -0.227  2.27\n",
    " -0.248 -0.103 -2.93   1.671 -1.032  1.011  1.568 -1.506  0.908 -1.775\n",
    "  0.536  1.073  0.144 -1.114 -0.598 -0.64  -0.495 -1.424 -0.289  1.259]\n",
    "\n",
    "\n",
    "Vector difference (mean absolute error): 0.009988984240985902\n",
    "\n",
    "Average relative error: 0.04160369772987067\n",
    "\n",
    "Maximum relative error: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 2:\n",
    "###  For a positve vector:\n",
    "Processing vector with values:\n",
    "[0.842 0.564 0.53  0.306 0.552 1.177 0.127 0.373 0.325 0.024 1.546 1.958\n",
    " 0.467 0.3   0.05  0.922 0.715 0.041 2.099 1.996 0.097 0.81  1.928 0.05\n",
    " 1.063 0.562 0.257 0.074 1.155 1.052 0.429 1.142 0.131 0.013 0.644 0.715\n",
    " 2.019 0.478 0.384 0.812 0.365 0.229 0.724 0.124 0.753 0.4   0.176 0.206\n",
    " 0.662 0.171 1.264 1.29  0.197 0.169 0.184 0.874 1.28  0.812 0.64  0.115\n",
    " 0.789 0.249 0.041 0.124 0.527 0.994 0.189 1.081 0.68  0.09  1.544 0.187\n",
    " 0.613 1.349 0.851 0.272 0.454 2.093 0.354 0.769 2.432 0.326 0.598 0.465\n",
    " 0.008 0.992 0.449 1.031 0.368 0.304 0.68  0.604 1.092 0.097 0.192 0.385\n",
    " 0.937 0.405 0.431 0.654]\n",
    "\n",
    "\n",
    "dequantized_weights: [0.837 0.561 0.523 0.304 0.551 1.169 0.124 0.371 0.323 0.019 1.54  1.949\n",
    " 0.466 0.295 0.048 0.922 0.713 0.038 2.092 1.987 0.095 0.808 1.921 0.048\n",
    " 1.055 0.561 0.257 0.067 1.15  1.046 0.428 1.141 0.124 0.01  0.637 0.713\n",
    " 2.016 0.475 0.38  0.808 0.361 0.228 0.723 0.124 0.751 0.399 0.171 0.2\n",
    " 0.656 0.162 1.255 1.284 0.19  0.162 0.181 0.865 1.274 0.808 0.637 0.114\n",
    " 0.78  0.247 0.038 0.124 0.523 0.989 0.181 1.074 0.675 0.086 1.54  0.181\n",
    " 0.608 1.341 0.846 0.266 0.447 2.092 0.352 0.761 2.424 0.323 0.589 0.456\n",
    " 0.    0.989 0.447 1.027 0.361 0.304 0.675 0.599 1.084 0.095 0.19  0.38\n",
    " 0.932 0.399 0.428 0.647]\n",
    "\n",
    "\n",
    "Vector difference (mean absolute error): 0.00449026730327917\n",
    "\n",
    "\n",
    "Average relative error: 0.027712205461241517\n",
    "\n",
    "\n",
    "Maximum relative error: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 3:\n",
    "###  For a vector with number in rang [-10,10]:\n",
    "\n",
    "Processing vector with values:\n",
    "[-7.486  6.453  3.698  1.974 -8.849 -0.654  0.024  3.682 -7.765 -7.426\n",
    " -4.756  3.291 -0.414 -4.156 -0.233 -1.8   -0.76  -3.587  8.459  0.476\n",
    "  1.153 -7.993  0.367  6.308 -8.249  7.706  7.149 -4.739 -7.791 -8.182\n",
    " -8.857 -1.62   5.834 -5.853  9.697 -9.905  1.939 -9.207  7.317 -4.41\n",
    "  8.908 -0.757  7.405 -1.313  9.301  5.434  2.446  1.684 -0.564 -8.49\n",
    " -9.02   5.381  5.913  3.296  3.75  -5.363 -7.126  5.366  4.749  0.752\n",
    " -7.923  6.479 -6.492  4.873 -0.831 -5.933 -1.023 -6.624  7.518  8.685\n",
    "  6.823  7.717 -2.144 -7.221 -1.954 -2.586  5.122  8.938 -7.526  6.726\n",
    " -6.834  9.901 -4.333 -5.694 -1.824  5.601 -8.753  7.033 -7.669  3.91\n",
    " -5.535  6.585  7.296  8.445  4.108  1.842 -1.195 -8.319 -2.418  3.343]\n",
    "\n",
    "\n",
    "dequantized_weights: [-7.456  6.447  3.651  1.942 -8.777 -0.621  0.     3.651 -7.689 -7.379\n",
    " -4.738  3.262 -0.388 -4.117 -0.233 -1.786 -0.699 -3.573  8.388  0.466\n",
    "  1.087 -7.922  0.311  6.291 -8.233  7.689  7.146 -4.738 -7.767 -8.155\n",
    " -8.854 -1.553  5.825 -5.825  9.631 -9.864  1.864 -9.165  7.301 -4.35\n",
    "  8.854 -0.699  7.379 -1.243  9.243  5.359  2.408  1.631 -0.544 -8.466\n",
    " -9.01   5.359  5.903  3.262  3.728 -5.359 -7.068  5.359  4.738  0.699\n",
    " -7.922  6.447 -6.447  4.816 -0.777 -5.903 -1.01  -6.602  7.456  8.621\n",
    "  6.757  7.689 -2.097 -7.146 -1.942 -2.563  5.049  8.932 -7.456  6.68\n",
    " -6.757  9.864 -4.272 -5.67  -1.786  5.592 -8.699  6.99  -7.612  3.884\n",
    " -5.515  6.524  7.223  8.388  4.039  1.786 -1.165 -8.311 -2.408  3.34 ]\n",
    "\n",
    "\n",
    "Vector difference (mean absolute error): 0.03731790080447429\n",
    "\n",
    "\n",
    "Average relative error: 0.024350610271637874\n",
    "\n",
    "\n",
    "Maximum relative error: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output 4:\n",
    "###  For a small number vector:\n",
    "\n",
    "Processing vector with values:\n",
    "[-2.255e-07 -1.138e-06 -1.651e-06  3.693e-07  1.656e-06  1.146e-06\n",
    "  7.117e-07 -2.367e-07  9.144e-08 -3.804e-07 -9.244e-08 -3.330e-07\n",
    "  2.401e-07  8.202e-07 -7.476e-07  5.232e-07  1.132e-06 -1.039e-06\n",
    " -1.128e-07  5.556e-07 -2.252e-07 -1.458e-06 -1.388e-06  7.225e-07\n",
    "  5.900e-07 -1.102e-06 -5.305e-07  1.218e-07 -1.339e-06  5.616e-08\n",
    " -1.877e-06 -1.625e-07 -7.366e-08  1.911e-07 -5.086e-07 -1.099e-06\n",
    " -3.371e-07 -3.181e-07 -1.950e-06  1.368e-06  1.594e-07  2.152e-08\n",
    "  1.294e-06 -3.561e-07  1.483e-06  4.243e-07 -5.544e-07  1.357e-06\n",
    "  1.551e-06  2.650e-06 -9.056e-07  7.431e-07 -8.916e-07 -1.031e-06\n",
    "  1.240e-06 -8.245e-07 -6.069e-07 -5.353e-07  1.293e-06 -5.546e-07\n",
    "  3.173e-07  4.125e-07 -1.560e-06  7.750e-08 -2.279e-06  2.132e-06\n",
    " -1.090e-06  3.612e-07 -9.459e-07  2.980e-07 -7.555e-07  8.842e-07\n",
    "  6.081e-07  6.104e-07  4.233e-07 -3.547e-07 -1.242e-06 -1.045e-06\n",
    " -5.698e-07  1.037e-06 -1.407e-07 -4.682e-07  4.941e-07 -2.270e-07\n",
    " -3.758e-07  5.382e-07 -1.437e-06 -1.101e-06  3.997e-07  2.200e-06\n",
    "  4.046e-08 -4.211e-07  1.344e-06  4.561e-08 -1.512e-06 -2.675e-07\n",
    "  1.218e-07 -3.649e-07  4.414e-07  4.752e-07]\n",
    "\n",
    "\n",
    "dequantized_weights: [-2.126e-07 -1.121e-06 -1.643e-06  3.672e-07  1.643e-06  1.140e-06\n",
    "  6.958e-07 -2.319e-07  7.731e-08 -3.672e-07 -7.731e-08 -3.286e-07\n",
    "  2.319e-07  8.118e-07 -7.345e-07  5.219e-07  1.121e-06 -1.024e-06\n",
    " -9.664e-08  5.412e-07 -2.126e-07 -1.450e-06 -1.372e-06  7.152e-07\n",
    "  5.799e-07 -1.102e-06 -5.219e-07  1.160e-07 -1.334e-06  3.866e-08\n",
    " -1.875e-06 -1.546e-07 -5.799e-08  1.740e-07 -5.025e-07 -1.082e-06\n",
    " -3.286e-07 -3.093e-07 -1.933e-06  1.353e-06  1.546e-07  1.933e-08\n",
    "  1.276e-06 -3.479e-07  1.469e-06  4.059e-07 -5.412e-07  1.353e-06\n",
    "  1.546e-06  2.648e-06 -8.891e-07  7.345e-07 -8.891e-07 -1.024e-06\n",
    "  1.237e-06 -8.118e-07 -5.992e-07 -5.219e-07  1.276e-06 -5.412e-07\n",
    "  3.093e-07  4.059e-07 -1.546e-06  7.731e-08 -2.261e-06  2.126e-06\n",
    " -1.082e-06  3.479e-07 -9.278e-07  2.899e-07 -7.538e-07  8.698e-07\n",
    "  5.992e-07  5.992e-07  4.059e-07 -3.479e-07 -1.237e-06 -1.044e-06\n",
    " -5.605e-07  1.024e-06 -1.353e-07 -4.639e-07  4.832e-07 -2.126e-07\n",
    " -3.672e-07  5.219e-07 -1.430e-06 -1.082e-06  3.866e-07  2.184e-06\n",
    "  3.866e-08 -4.059e-07  1.334e-06  3.866e-08 -1.508e-06 -2.513e-07\n",
    "  1.160e-07 -3.479e-07  4.252e-07  4.639e-07]\n",
    "\n",
    "Vector difference (mean absolute error): 1.0179261771897355e-08\n",
    "\n",
    "Average relative error: 0.030824232498440164\n",
    "\n",
    "Maximum relative error: 0.311715113852703\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6:\n",
    "### Compare between INT8 and INT16 QUANTIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization analysis completed. Results saved to 'quantization_16bit_vs_8bit.txt'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate test vectors\n",
    "vectors = [\n",
    "    np.random.randn(1000).astype(np.float32),  # Normal distribution\n",
    "    np.random.uniform(-1, 1, 1000).astype(np.float32),  # Uniform distribution\n",
    "    np.linspace(-10, 10, 1000).astype(np.float32)  # Linear ramp\n",
    "]\n",
    "\n",
    "# Open a file for saving results\n",
    "with open('quantization_16bit_vs_8bit.txt', 'w') as file:\n",
    "    for vec in vectors:\n",
    "        file.write(f\"Processing vector with first values: {vec[:5]}...\\n\")\n",
    "\n",
    "        # Generate grids for both 16-bit and 8-bit\n",
    "        grid_16bit = quantizationItamar.generate_grid(16, False)\n",
    "        grid_8bit = quantizationItamar.generate_grid(8, False)\n",
    "\n",
    "        try:\n",
    "            # Perform 16-bit quantization\n",
    "            quantized_16bit, scale_16bit, z_16bit = Quantizer.quantize(vec, grid_16bit)\n",
    "            dequantized_16bit = Quantizer.dequantize(quantized_16bit, scale_16bit, z_16bit)\n",
    "\n",
    "            # Perform 8-bit quantization\n",
    "            quantized_8bit, scale_8bit, z_8bit = Quantizer.quantize(vec, grid_8bit)\n",
    "            dequantized_8bit = Quantizer.dequantize(quantized_8bit, scale_8bit, z_8bit)\n",
    "\n",
    "            # Compute errors\n",
    "            error_16bit = np.abs(vec - dequantized_16bit).mean()\n",
    "            max_error_16bit = np.abs(vec - dequantized_16bit).max()\n",
    "\n",
    "            error_8bit = np.abs(vec - dequantized_8bit).mean()\n",
    "            max_error_8bit = np.abs(vec - dequantized_8bit).max()\n",
    "\n",
    "            # Write results\n",
    "            file.write(f\"16-bit Quantization -> Mean Error: {error_16bit}, Max Error: {max_error_16bit}\\n\")\n",
    "            file.write(f\"8-bit Quantization  -> Mean Error: {error_8bit}, Max Error: {max_error_8bit}\\n\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            file.write(f\"Error processing vector: {str(e)}\\n\\n\")\n",
    "\n",
    "print(\"Quantization analysis completed. Results saved to 'quantization_16bit_vs_8bit.txt'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results of this conpare:\n",
    "### First vector:\n",
    "Processing vector with first values: [ 0.255  0.02   1.868 -0.964  0.563]...\n",
    "16-bit Quantization : \n",
    "\n",
    "Mean Error: 5.056971670431684e-05\n",
    "\n",
    "Max Error: 0.00010206876043378221\n",
    "\n",
    "8-bit Quantization:\n",
    "\n",
    " Mean Error: 0.013173053463762753 \n",
    "\n",
    " Max Error: 0.026206773870131705\n",
    "\n",
    "### second vector:\n",
    "Processing vector with first values: [-0.813 -0.25  -0.583  0.722 -0.494]...\n",
    "\n",
    "16-bit Quantization:\n",
    "\n",
    "Mean Error: 1.5144269051737838e-05\n",
    "\n",
    "Max Error: 3.0485269083280198e-05\n",
    "\n",
    "8-bit Quantization:\n",
    "\n",
    "Mean Error: 0.003858430505117119\n",
    "\n",
    "Max Error: 0.007832529147466072\n",
    "\n",
    "### thired vector\n",
    "Processing vector with first values: [-10.    -9.98  -9.96  -9.94  -9.92]...\n",
    "\n",
    "16-bit Quantization:\n",
    "\n",
    "Mean Error: 0.0001477456901089138\n",
    "\n",
    "Max Error: 0.00030483151406279774\n",
    "\n",
    "8-bit Quantization: \n",
    "\n",
    "Mean Error: 0.039514801461834\n",
    "\n",
    "Max Error: 0.07831359552402123\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
